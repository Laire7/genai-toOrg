{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojoyvz6mH1Hv"
      },
      "source": [
        "# Supervised Fine Tuning with Gemini 1.5 Flash for Q&A\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/supervised_finetuning_using_gemini_qa.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Ftuning%2Fsupervised_finetuning_using_gemini_qa.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/tuning/supervised_finetuning_using_gemini_qa.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/supervised_finetuning_using_gemini_qa.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Erwin Huizenga](https://github.com/erwinh85) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "## Overview\n",
        "\n",
        "**Gemini** is a family of generative AI models developed by Google DeepMind designed for multimodal use cases. The Gemini API gives you access to the various Gemini models, such as Gemini 1.5 Pro and Gemini 1.5 Flash.\n",
        "This notebook demonstrates fine-tuning the Gemini 1.5 Flahs using the Vertex AI Supervised Tuning feature. Supervised Tuning allows you to use your own labeled training data to further refine the base model's capabilities toward your specific tasks.\n",
        "Supervised Tuning uses labeled examples to tune a model. Each example demonstrates the output you want from your text model during inference.\n",
        "First, ensure your training data is of high quality, well-labeled, and directly relevant to the target task. This is crucial as low-quality data can adversely affect the performance and introduce bias in the fine-tuned model.\n",
        "Training: Experiment with different configurations to optimize the model's performance on the target task.\n",
        "Evaluation:\n",
        "Metric: Choose appropriate evaluation metrics that accurately reflect the success of the fine-tuned model for your specific task\n",
        "Evaluation Set: Use a separate set of data to evaluate the model's performance\n",
        "\n",
        "\n",
        "Refer to public [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning) for more details.\n",
        "\n",
        "<hr/>\n",
        "\n",
        "Before running this notebook, ensure you have:\n",
        "\n",
        "- A Google Cloud project: Provide your project ID in the `PROJECT_ID` variable.\n",
        "\n",
        "- Authenticated your Colab environment: Run the authentication code block at the beginning.\n",
        "\n",
        "- Prepared training data (Test with your own data or use the one in the notebook): Data should be formatted in JSONL with prompts and corresponding completions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7SS5pzuIA-1"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage.\n",
        "\n",
        "To get an estimate of the number of tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --user --quiet google-cloud-aiplatform datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import os\n",
        "\n",
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"<your_project_id>\"  # @param {type:\"string\", isTemplate: true}\n",
        "if PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5303c05f7aa6"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fc324893334"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "\n",
        "# Vertex AI SDK\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform.metadata import context\n",
        "from google.cloud.aiplatform.metadata import utils as metadata_utils\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Vertex AI SDK\n",
        "from sklearn.metrics import f1_score\n",
        "from vertexai.generative_models import GenerationConfig, GenerativeModel\n",
        "from vertexai.preview.tuning import sft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bBZa2I-c-x8"
      },
      "source": [
        "### Data\n",
        "\n",
        "#### SQuAD dataset\n",
        "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
        "\n",
        "You can fine more information on the SQuAD [github page](https://rajpurkar.github.io/SQuAD-explorer/)**bold text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhebDJjRKePL"
      },
      "source": [
        "First update the `BUCKET_NAME` parameter below. You can either use an existing bucket or create a new one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lit30Cktbfvo"
      },
      "outputs": [],
      "source": [
        "# Provide a bucket name\n",
        "BUCKET_NAME = \"<your_gcs_bucket>\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "print(BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed-G-9cyKmPY"
      },
      "source": [
        "Only run the code below if you want to create a new Google Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UJ8S9YFA1pZ"
      },
      "outputs": [],
      "source": [
        "# ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izjwF63tLLEq"
      },
      "source": [
        "Next you will copy the data into your bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjvcxx_sA3xP"
      },
      "outputs": [],
      "source": [
        "!gsutil cp gs://github-repo/generative-ai/gemini/tuning/qa/squad_test.csv .\n",
        "!gsutil cp gs://github-repo/generative-ai/gemini/tuning/qa/squad_train.csv .\n",
        "!gsutil cp gs://github-repo/generative-ai/gemini/tuning/qa/squad_validation.csv ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F10LuZeL3kt"
      },
      "source": [
        "### Baseline\n",
        "\n",
        "Next you will prepare some test data that you will use to establish a baseline.  This means evaluating your chosen model on a representative sample of your dataset before any fine-tuning. A baseline allows you to quantify the improvements achieved through fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkOmXpegA8CW"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"squad_test.csv\")\n",
        "test_df.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrLxcVVcMsNO"
      },
      "source": [
        "You will need to do some dataset preperations. We will add a system instruction to the dataset:\n",
        "\n",
        "`SystemInstruct`: System instructions are a set of instructions that the model processes before it processes prompts. We recommend that you use system instructions to tell the model how you want it to behave and respond to prompts.\n",
        "\n",
        "We will also combine the `context` and `question`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0pgJycOekZ3"
      },
      "outputs": [],
      "source": [
        "systemInstruct = \"Answer the question based on the context\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_u3VzUMsyqj"
      },
      "outputs": [],
      "source": [
        "# combine the systeminstruct + context + question into one column.\n",
        "row_dataset = 6\n",
        "\n",
        "test_df[\"input_question\"] = (\n",
        "    systemInstruct\n",
        "    + \"\\n\"\n",
        "    + \"Context: \"\n",
        "    + test_df[\"context\"]\n",
        "    + \"\\n\"\n",
        "    + \"Question: \"\n",
        "    + test_df[\"question\"]\n",
        ")\n",
        "test_question = test_df[\"input_question\"].iloc[row_dataset]\n",
        "print(test_question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSxYYqMGWrmj"
      },
      "source": [
        "Next, set the model that you will use. In this example you will use `gemini-1.5-flash-002`. A multimodal model that is designed for high-volume, cost-effective applications, and which delivers speed and efficiency to build fast, lower-cost applications that don't compromise on quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-5X4goiqqBQ"
      },
      "outputs": [],
      "source": [
        "base_model = \"gemini-1.5-flash-002\"\n",
        "generation_model = GenerativeModel(base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyscyIenW4WZ"
      },
      "source": [
        "Next lets take a question and get a prediction from Gemini that we can compare to the actual answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejjhfynQWc0k"
      },
      "outputs": [],
      "source": [
        "y_true = test_df[\"answers\"].values\n",
        "y_pred_question = test_df[\"question\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXencUYc6YAE"
      },
      "outputs": [],
      "source": [
        "def get_predictions(question: str) -> str:\n",
        "    \"\"\"Generates predictions for a given test question.\n",
        "\n",
        "    Args:\n",
        "      test_question: The question to generate predictions for.\n",
        "\n",
        "    Returns:\n",
        "      The generated prediction text.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"{question}\"\n",
        "\n",
        "    generation_config = GenerationConfig(temperature=0.1)\n",
        "\n",
        "    response = generation_model.generate_content(\n",
        "        contents=prompt, generation_config=generation_config\n",
        "    ).text\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKa0wLooa3Is"
      },
      "outputs": [],
      "source": [
        "test_answer = test_df[\"answers\"].iloc[row_dataset]\n",
        "\n",
        "response = get_predictions(test_question)\n",
        "\n",
        "print(f\"Gemini response: {response}\")\n",
        "print(f\"Actual answer: {test_answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGRJTHKrdujw"
      },
      "source": [
        "You can see that both answers are correct, but the response from Gemini is more lengthy. However, answers in the SQuAD dataset are typically concise and clear.\n",
        "\n",
        "Fine-tuning is a great way to control the type of output your use case requires. In this instance, you would want the model to provide short, clear answers.\n",
        "\n",
        "Next, let's check if each dataset has an equal number of examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCe0CUsi5E-Y"
      },
      "outputs": [],
      "source": [
        "y_pred = test_df[\"question\"].values\n",
        "\n",
        "num_strings_pred = np.sum([isinstance(item, str) for item in y_pred])\n",
        "print(f\"Number of strings in y_pred: {num_strings_pred}\")\n",
        "\n",
        "num_strings_true = np.sum([isinstance(item, str) for item in y_true])\n",
        "print(f\"Number of strings in y_true: {num_strings_true}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvi7m8pKE8WB"
      },
      "source": [
        "Next lest establish a baseline using evaluation metrics.\n",
        "\n",
        "Evaluating the performance of a Question Answering (QA) system requires specific metrics. Two commonly used metrics are Exact Match (EM) and F1 score.\n",
        "\n",
        "EM is a strict measure that only considers an answer correct if it perfectly matches the ground truth, even down to the punctuation. It's a binary metric - either 1 for a perfect match or 0 otherwise. This makes it sensitive to minor variations in phrasing.\n",
        "\n",
        "F1 score is more flexible. It considers the overlap between the predicted answer and the true answer in terms of individual words or tokens. It calculates the harmonic mean of precision (proportion of correctly predicted words out of all predicted words) and recall (proportion of correctly predicted words out of all true answer words). This allows for partial credit and is less sensitive to minor wording differences.\n",
        "\n",
        "In practice, EM is useful when exact wording is crucial, while F1 is more suitable when evaluating the overall understanding and semantic accuracy of the QA system. Often, both metrics are used together to provide a comprehensive evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcgEpTU55FFc"
      },
      "outputs": [],
      "source": [
        "def calculate_em_and_f1_for_text_arrays(y_true, y_pred, average=\"weighted\"):\n",
        "    \"\"\"\n",
        "    Calculates the Exact Match (EM) and F1 score for arrays of text\n",
        "    using word-level comparisons.\n",
        "\n",
        "    Args:\n",
        "        y_true: An array of ground truth strings.\n",
        "        y_pred: An array of predicted strings.\n",
        "        average: The averaging method to use for F1 score.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the EM score and the F1 score.\n",
        "    \"\"\"\n",
        "\n",
        "    em = np.mean([t == p for t, p in zip(y_true, y_pred)])\n",
        "\n",
        "    # Use TF-IDF to convert strings to numerical vectors\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    all_text = np.concatenate((y_true, y_pred))\n",
        "    vectorizer.fit(all_text)\n",
        "    y_true_vec = vectorizer.transform(y_true)\n",
        "    y_pred_vec = vectorizer.transform(y_pred)\n",
        "\n",
        "    # Calculate F1 score based on common words (non-zero elements)\n",
        "    y_true_class = (y_true_vec > 0).toarray().astype(int)\n",
        "    y_pred_class = (y_pred_vec > 0).toarray().astype(int)\n",
        "\n",
        "    f1 = f1_score(y_true_class, y_pred_class, average=average)\n",
        "\n",
        "    return em, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhDTq9p_GSBP"
      },
      "outputs": [],
      "source": [
        "em, f1 = calculate_em_and_f1_for_text_arrays(y_pred, y_true)\n",
        "print(f\"EM score: {em}\")\n",
        "print(f\"F1 score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22DfexbNfUHm"
      },
      "source": [
        "### Prepare the data for fine-tuning\n",
        "\n",
        "To optimize the tuning process for a foundation model, ensure your dataset includes examples that reflect the desired task. Structure your training data in a text-to-text format, where each record in the dataset pairs an input text (or prompt) with its corresponding expected output. This supervised tuning approach uses the dataset to effectively teach the model the specific behavior or task you need it to perform, by providing numerous illustrative examples.\n",
        "\n",
        "The size of your dataset will vary depending on the complexity of the task, but as a general rule, the more examples you include, the better the model's performance.\n",
        "\n",
        "Dataset Format\n",
        "Your training data should be structured in a JSONL file and stored at a Google Cloud Storage (GCS) URI.  Each line in the JSONL file must adhere to the following schema:\n",
        "\n",
        "A `contents` array containing objects that define:\n",
        "- A `role` (\"user\" for user input or \"model\" for model output)\n",
        "- `parts` containing the input data.\n",
        "\n",
        "```\n",
        "{\n",
        "   \"contents\":[\n",
        "      {\n",
        "         \"role\":\"user\",  # This indicate input content\n",
        "         \"parts\":[\n",
        "            {\n",
        "               \"text\":\"How are you?\"\n",
        "            }\n",
        "         ]\n",
        "      },\n",
        "      {\n",
        "         \"role\":\"model\", # This indicate target content\n",
        "         \"parts\":[ # text only\n",
        "            {\n",
        "               \"text\":\"I am good, thank you!\"\n",
        "            }\n",
        "         ]\n",
        "      }\n",
        "      #  ... repeat \"user\", \"model\" for multi turns.\n",
        "   ]\n",
        "}\n",
        "```\n",
        "\n",
        "Refer to the public [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning-prepare#about-datasets) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DqrQp4cLqRy"
      },
      "outputs": [],
      "source": [
        "# combine the systeminstruct + context + question into one column.\n",
        "train_df = pd.read_csv(\"squad_train.csv\")\n",
        "validation_df = pd.read_csv(\"squad_validation.csv\")\n",
        "\n",
        "train_df[\"input_question\"] = (\n",
        "    systemInstruct\n",
        "    + \"Context: \"\n",
        "    + train_df[\"context\"]\n",
        "    + \"Question: \"\n",
        "    + train_df[\"question\"]\n",
        ")\n",
        "validation_df[\"input_question\"] = (\n",
        "    systemInstruct\n",
        "    + \"Context: \"\n",
        "    + validation_df[\"context\"]\n",
        "    + \"Question: \"\n",
        "    + validation_df[\"question\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pmzyz1migvHN"
      },
      "outputs": [],
      "source": [
        "def df_to_jsonl(df, output_file):\n",
        "    \"\"\"Converts a Pandas DataFrame to JSONL format and saves it to a file.\n",
        "\n",
        "    Args:\n",
        "      df: The DataFrame to convert.\n",
        "      output_file: The name of the output file.\n",
        "    \"\"\"\n",
        "\n",
        "    with open(output_file, \"w\") as f:\n",
        "        for row in df.itertuples(index=False):\n",
        "            jsonl_obj = {\n",
        "                \"systemInstruction\": {\n",
        "                    \"parts\": [\n",
        "                        {\"text\": \"Answer the question based on the provided context.\"}\n",
        "                    ]\n",
        "                },\n",
        "                \"contents\": [\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"parts\": [\n",
        "                            {\n",
        "                                \"text\": f\"Context: {row.context}\\n\\nQuestion: {row.question}\"\n",
        "                            }\n",
        "                        ],\n",
        "                    },\n",
        "                    {\"role\": \"model\", \"parts\": [{\"text\": row.answers}]},\n",
        "                ],\n",
        "            }\n",
        "            f.write(json.dumps(jsonl_obj) + \"\\n\")\n",
        "\n",
        "\n",
        "# Process the DataFrames\n",
        "df_to_jsonl(train_df, \"squad_train.jsonl\")\n",
        "df_to_jsonl(validation_df, \"squad_validation.jsonl\")\n",
        "\n",
        "print(f\"JSONL data written to squad_train.jsonl\")\n",
        "print(f\"JSONL data written to squad_validation.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OQv-ZMpJDhi"
      },
      "source": [
        "Next you will copy the files into your Google Cloud bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5k1jYJ10IeW"
      },
      "outputs": [],
      "source": [
        "!gsutil cp ./squad_train.jsonl {BUCKET_URI}\n",
        "!gsutil cp ./squad_validation.jsonl {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAHMYgFJJHjm"
      },
      "source": [
        "### Start fine-tuning job\n",
        "Next you can start the fine-tuning job.\n",
        "\n",
        "- `source_model`: Specifies the base Gemini model version you want to fine-tune.\n",
        " - `train_dataset`: Path to your training data in JSONL format.\n",
        "\n",
        "  *Optional parameters*\n",
        " - `validation_dataset`: If provided, this data is used to evaluate the model during tuning.\n",
        " - `tuned_model_display_name`: Display name for the tuned model.\n",
        " - `epochs`: The number of training epochs to run.\n",
        " - `learning_rate_multiplier`: A value to scale the learning rate during training.\n",
        " - `adapter_size` : Gemini 1.5 Flash supports Adapter length [1, 4], default value is 4.\n",
        "\n",
        " **Important**: The default hyperparameter settings are optimized for optimal performance based on rigorous testing and are recommended for initial use. Users may customize these parameters to address specific performance requirements.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qj-LjQ5Vbf1E"
      },
      "outputs": [],
      "source": [
        "tuned_model_display_name = \"fine-tuning-gemini-flash-qa-v01\"  # @param {type:\"string\"}\n",
        "\n",
        "sft_tuning_job = sft.train(\n",
        "    source_model=base_model,\n",
        "    train_dataset=f\"\"\"{BUCKET_URI}/squad_train.jsonl\"\"\",\n",
        "    # # Optional:\n",
        "    validation_dataset=f\"\"\"{BUCKET_URI}/squad_validation.jsonl\"\"\",\n",
        "    tuned_model_display_name=tuned_model_display_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tXawW1p8E5-"
      },
      "outputs": [],
      "source": [
        "# Get the tuning job info.\n",
        "sft_tuning_job.to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19aQnN-k84d9"
      },
      "outputs": [],
      "source": [
        "# Get the resource name of the tuning job\n",
        "sft_tuning_job_name = sft_tuning_job.resource_name\n",
        "sft_tuning_job_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKo8cwF2KVM5"
      },
      "source": [
        "**Important:** Tuning time depends on several factors, such as training data size, number of epochs, learning rate multiplier, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NiZnPkIKcwm"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ It will take ~30 mins for the model tuning job to complete on the provided dataset and set configurations/hyperparameters. ⚠️</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Njag_3cB86rH"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Wait for job completion\n",
        "while not sft_tuning_job.refresh().has_ended:\n",
        "    time.sleep(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkx92RBdbf27"
      },
      "outputs": [],
      "source": [
        "# tuned model name\n",
        "tuned_model_name = sft_tuning_job.tuned_model_name\n",
        "tuned_model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e09aB_9Ebf5c"
      },
      "outputs": [],
      "source": [
        "# tuned model endpoint name\n",
        "tuned_model_endpoint_name = sft_tuning_job.tuned_model_endpoint_name\n",
        "tuned_model_endpoint_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1ukBznKmlN"
      },
      "source": [
        "#### Model tuning metrics\n",
        "\n",
        "- `/train_total_loss`: Loss for the tuning dataset at a training step.\n",
        "- `/train_fraction_of_correct_next_step_preds`: The token accuracy at a training step. A single prediction consists of a sequence of tokens. This metric measures the accuracy of the predicted tokens when compared to the ground truth in the tuning dataset.\n",
        "- `/train_num_predictions`: Number of predicted tokens at a training step\n",
        "\n",
        "#### Model evaluation metrics:\n",
        "\n",
        "- `/eval_total_loss`: Loss for the evaluation dataset at an evaluation step.\n",
        "- `/eval_fraction_of_correct_next_step_preds`: The token accuracy at an evaluation step. A single prediction consists of a sequence of tokens. This metric measures the accuracy of the predicted tokens when compared to the ground truth in the evaluation dataset.\n",
        "- `/eval_num_predictions`: Number of predicted tokens at an evaluation step.\n",
        "\n",
        "The metrics visualizations are available after the model tuning job completes. If you don't specify a validation dataset when you create the tuning job, only the visualizations for the tuning metrics are available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHVU4XP2aOFE"
      },
      "outputs": [],
      "source": [
        "# Get resource name from tuning job.\n",
        "experiment_name = sft_tuning_job.experiment.resource_name\n",
        "experiment_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH0guHM---Jo"
      },
      "outputs": [],
      "source": [
        "# Locate Vertex AI Experiment and Vertex AI Experiment Run\n",
        "experiment = aiplatform.Experiment(experiment_name=experiment_name)\n",
        "filter_str = metadata_utils._make_filter_string(\n",
        "    schema_title=\"system.ExperimentRun\",\n",
        "    parent_contexts=[experiment.resource_name],\n",
        ")\n",
        "experiment_run = context.Context.list(filter_str)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hggHQFIl_FXC"
      },
      "outputs": [],
      "source": [
        "# Read data from Tensorboard\n",
        "tensorboard_run_name = f\"{experiment.get_backing_tensorboard_resource().resource_name}/experiments/{experiment.name}/runs/{experiment_run.name.replace(experiment.name, '')[1:]}\"\n",
        "tensorboard_run = aiplatform.TensorboardRun(tensorboard_run_name)\n",
        "metrics = tensorboard_run.read_time_series_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdHKZdqG_bHf"
      },
      "outputs": [],
      "source": [
        "def get_metrics(metric: str = \"/train_total_loss\"):\n",
        "    \"\"\"\n",
        "    Get metrics from Tensorboard.\n",
        "\n",
        "    Args:\n",
        "      metric: metric name, eg. /train_total_loss or /eval_total_loss.\n",
        "    Returns:\n",
        "      steps: list of steps.\n",
        "      steps_loss: list of loss values.\n",
        "    \"\"\"\n",
        "    loss_values = metrics[metric].values\n",
        "    steps_loss = []\n",
        "    steps = []\n",
        "    for loss in loss_values:\n",
        "        steps_loss.append(loss.scalar.value)\n",
        "        steps.append(loss.step)\n",
        "    return steps, steps_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pDrlpA7_e9o"
      },
      "outputs": [],
      "source": [
        "# Get Train and Eval Loss\n",
        "train_loss = get_metrics(metric=\"/train_total_loss\")\n",
        "eval_loss = get_metrics(metric=\"/eval_total_loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL07j7u__iZx"
      },
      "outputs": [],
      "source": [
        "# Plot the train and eval loss metrics using Plotly python library\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2, shared_xaxes=True, subplot_titles=(\"Train Loss\", \"Eval Loss\")\n",
        ")\n",
        "\n",
        "# Add traces\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=train_loss[0], y=train_loss[1], name=\"Train Loss\", mode=\"lines\"),\n",
        "    row=1,\n",
        "    col=1,\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=eval_loss[0], y=eval_loss[1], name=\"Eval Loss\", mode=\"lines\"),\n",
        "    row=1,\n",
        "    col=2,\n",
        ")\n",
        "\n",
        "# Add figure title\n",
        "fig.update_layout(title=\"Train and Eval Loss\", xaxis_title=\"Steps\", yaxis_title=\"Loss\")\n",
        "\n",
        "# Set x-axis title\n",
        "fig.update_xaxes(title_text=\"Steps\")\n",
        "\n",
        "# Set y-axes titles\n",
        "fig.update_yaxes(title_text=\"Loss\")\n",
        "\n",
        "# Show plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pivmh4Lwbgy1"
      },
      "source": [
        "### Use the fine-tuned model and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO6ln4teagw1"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Answer the question based on the context\n",
        "\n",
        "Context: In the 1840s and 50s, there were attempts to overcome this problem by means of various patent valve gears with a separate, variable cutoff expansion valve riding on the back of the main slide valve; the latter usually had fixed or limited cutoff.\n",
        "The combined setup gave a fair approximation of the ideal events, at the expense of increased friction and wear, and the mechanism tended to be complicated.\n",
        "The usual compromise solution has been to provide lap by lengthening rubbing surfaces of the valve in such a way as to overlap the port on the admission side, with the effect that the exhaust side remains open for a longer period after cut-off on the admission side has occurred.\n",
        "This expedient has since been generally considered satisfactory for most purposes and makes possible the use of the simpler Stephenson, Joy and Walschaerts motions.\n",
        "Corliss, and later, poppet valve gears had separate admission and exhaust valves driven by trip mechanisms or cams profiled so as to give ideal events; most of these gears never succeeded outside of the stationary marketplace due to various other issues including leakage and more delicate mechanisms.\n",
        "\n",
        "Question: How is lap provided by overlapping the admission side port?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYygz5ph_icf"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    tuned_genai_model = GenerativeModel(tuned_model_endpoint_name)\n",
        "    # Test with the loaded model.\n",
        "    print(\"***Testing***\")\n",
        "    print(tuned_genai_model.generate_content(contents=prompt))\n",
        "else:\n",
        "    print(\"State:\", sft_tuning_job.state)\n",
        "    print(\"Error:\", sft_tuning_job.error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4YMNGuoDajB"
      },
      "outputs": [],
      "source": [
        "y_true = test_df[\"answers\"].values\n",
        "\n",
        "\n",
        "def get_predictions(test_question):\n",
        "    prompt = f\"\"\"{test_question}\"\"\"\n",
        "\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=0.1,\n",
        "    )\n",
        "\n",
        "    response = tuned_genai_model.generate_content(\n",
        "        contents=prompt, generation_config=generation_config\n",
        "    ).text\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69FMuAeoDrm5"
      },
      "outputs": [],
      "source": [
        "y_pred = []\n",
        "y_pred_question = test_df[\"question\"].values\n",
        "\n",
        "for i in y_pred_question:\n",
        "    prediction = get_predictions(i)\n",
        "    y_pred.append(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj76Tu6ODalZ"
      },
      "outputs": [],
      "source": [
        "em, f1 = calculate_em_and_f1_for_text_arrays(y_pred, y_true)\n",
        "print(f\"EM score: {em}\")\n",
        "print(f\"F1 score: {f1}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "supervised_finetuning_using_gemini_qa.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
